{
    "logistic-regression-word-1_2-char-2_6": {
        "class": "LogisticRegressionBagOfNgrams",
        "params": {
            "model_params": {},
            "tokenizer_params": {
                "word": {
                    "ngram_range": [
                        1,
                        2
                    ],
                    "max_features": 5000
                },
                "char": {
                    "ngram_range": [
                        2,
                        6
                    ],
                    "max_features": 5000
                }
            },
            "training_params": {},
            "inference_params": {}
        }
    },
    "logistic-regression-readability": {
        "class": "LogisticRegressionReadability",
        "params": {
            "model_params": {},
            "tokenizer_params": {},
            "training_params": {},
            "inference_params": {}
        }
    },
    "multilingual-dec-512-shots": {
        "class": "SymantoDualEncoder",
        "params": {
            "model_params": {
                "pretrained_model_name_or_path": "paraphrase-multilingual-mpnet-base-v2",
                "label2text": {
                    "b": "This text has been generated by A.",
                    "c": "This text has been generated by B.",
                    "d": "This text has been generated by C.",
                    "e": "This text has been generated by D.",
                    "a": "This text has been generated by E.",
                    "f": "This text has been generated by F."
                }
            },
            "tokenizer_params": {},
            "training_params": {
                "shots": 512,
                "batch_size": 4,
                "epochs": 5,
                "lr": 2e-5,
                "strategy": "default"
            },
            "inference_params": {}
        }
    },
    "multilingual-dec-zero-shot": {
        "class": "SymantoDualEncoder",
        "params": {
            "model_params": {
                "pretrained_model_name_or_path": "paraphrase-multilingual-mpnet-base-v2",
                "label2text": {
                    "b": "This text has been generated by A.",
                    "c": "This text has been generated by B.",
                    "d": "This text has been generated by C.",
                    "e": "This text has been generated by D.",
                    "a": "This text has been generated by E.",
                    "f": "This text has been generated by F."
                }
            },
            "tokenizer_params": {},
            "training_params": {
                "shots": 0
            },
            "inference_params": {}
        }
    },
    "xlm-roberta-large": {
        "class": "HuggingFaceClassifier",
        "params": {
            "model_params": {
                "pretrained_model_name_or_path": "FacebookAI/xlm-roberta-large",
                "num_labels": 6,
                "id2label": {
                    "0": "b",
                    "1": "c",
                    "2": "d",
                    "3": "e",
                    "4": "a",
                    "5": "f"
                },
                "label2id": {
                    "c": 0,
                    "b": 1,
                    "d": 2,
                    "e": 3,
                    "a": 4,
                    "f": 5
                }
            },
            "tokenizer_params": {
                "model_max_length": 512
            },
            "training_params": {
                "output_dir": "./checkpoints/subtask_2/xlm_roberta",
                "per_device_train_batch_size": 4,
                "num_train_epochs": 5,
                "learning_rate": 5e-5,
                "logging_steps": 20,
                "save_strategy": "no",
                "fp16": true,
                "auto_find_batch_size": true
            },
            "inference_params": {
                "output_dir": "./checkpoints/subtask_2/xlm_roberta",
                "per_device_eval_batch_size": 4
            }
        }
    },
    "mdeberta-v3-large": {
        "class": "HuggingFaceClassifier",
        "params": {
            "model_params": {
                "pretrained_model_name_or_path": "microsoft/mdeberta-v3-base",
                "num_labels": 6,
                "id2label": {
                    "0": "b",
                    "1": "c",
                    "2": "d",
                    "3": "e",
                    "4": "a",
                    "5": "f"
                },
                "label2id": {
                    "c": 0,
                    "b": 1,
                    "d": 2,
                    "e": 3,
                    "a": 4,
                    "f": 5
                }
            },
            "tokenizer_params": {
                "model_max_length": 512
            },
            "training_params": {
                "output_dir": "./checkpoints/subtask_2/mdeberta-v3-large",
                "per_device_train_batch_size": 4,
                "num_train_epochs": 5,
                "learning_rate": 5e-5,
                "logging_steps": 20,
                "save_strategy": "no",
                "fp16": true,
                "auto_find_batch_size": true
            },
            "inference_params": {
                "output_dir": "./checkpoints/subtask_2/mdeberta-v3-large",
                "per_device_eval_batch_size": 4
            }
        }
    },
    "mbart-large-50": {
        "class": "HuggingFaceClassifier",
        "params": {
            "model_params": {
                "pretrained_model_name_or_path": "facebook/mbart-large-50",
                "num_labels": 6,
                "id2label": {
                    "0": "b",
                    "1": "c",
                    "2": "d",
                    "3": "e",
                    "4": "a",
                    "5": "f"
                },
                "label2id": {
                    "c": 0,
                    "b": 1,
                    "d": 2,
                    "e": 3,
                    "a": 4,
                    "f": 5
                }
            },
            "tokenizer_params": {
                "model_max_length": 512
            },
            "training_params": {
                "output_dir": "./checkpoints/subtask_2/mbart-large-50",
                "per_device_train_batch_size": 4,
                "num_train_epochs": 5,
                "learning_rate": 5e-5,
                "logging_steps": 20,
                "save_strategy": "no",
                "fp16": true,
                "auto_find_batch_size": true
            },
            "inference_params": {
                "output_dir": "./checkpoints/subtask_2/mbart-large-50",
                "per_device_eval_batch_size": 4
            }
        }
    },
    "mt5-large": {
        "class": "HuggingFaceClassifier",
        "params": {
            "model_params": {
                "pretrained_model_name_or_path": "google/mt5-large",
                "num_labels": 6,
                "id2label": {
                    "0": "b",
                    "1": "c",
                    "2": "d",
                    "3": "e",
                    "4": "a",
                    "5": "f"
                },
                "label2id": {
                    "c": 0,
                    "b": 1,
                    "d": 2,
                    "e": 3,
                    "a": 4,
                    "f": 5
                }
            },
            "tokenizer_params": {
                "model_max_length": 512
            },
            "training_params": {
                "output_dir": "./checkpoints/subtask_2/mt5-large",
                "per_device_train_batch_size": 4,
                "num_train_epochs": 5,
                "learning_rate": 5e-5,
                "logging_steps": 20,
                "save_strategy": "no",
                "fp16": true,
                "auto_find_batch_size": true
            },
            "inference_params": {
                "output_dir": "./checkpoints/subtask_2/mt5-large",
                "per_device_eval_batch_size": 4
            }
        }
    },
    "me5-large": {
        "class": "HuggingFaceClassifier",
        "params": {
            "model_params": {
                "pretrained_model_name_or_path": "intfloat/multilingual-e5-large",
                "num_labels": 6,
                "id2label": {
                    "0": "b",
                    "1": "c",
                    "2": "d",
                    "3": "e",
                    "4": "a",
                    "5": "f"
                },
                "label2id": {
                    "c": 0,
                    "b": 1,
                    "d": 2,
                    "e": 3,
                    "a": 4,
                    "f": 5
                }
            },
            "tokenizer_params": {
                "model_max_length": 512
            },
            "training_params": {
                "output_dir": "./checkpoints/subtask_2/me5-large",
                "per_device_train_batch_size": 4,
                "num_train_epochs": 5,
                "learning_rate": 5e-5,
                "logging_steps": 20,
                "save_strategy": "no",
                "fp16": true,
                "auto_find_batch_size": true
            },
            "inference_params": {
                "output_dir": "./checkpoints/subtask_2/me5-large",
                "per_device_eval_batch_size": 4
            }
        }
    }
}